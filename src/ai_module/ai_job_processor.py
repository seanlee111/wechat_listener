"""
AIä½œä¸šå¤„ç†å™¨ - ä¸»æ§åˆ¶å™¨
æ•´åˆä¸‰é˜¶æ®µæ¶æ„ï¼šæ•°æ®å‡†å¤‡ â†’ æ™ºèƒ½æå– â†’ ç»“æœè¾“å‡º
"""

import logging
import time
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).resolve().parent
project_root = current_dir.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å¯¼å…¥ä¸‰é˜¶æ®µç»„ä»¶
from src.ai_module.data_pipeline import DataPipeline
from src.ai_module.llm_extractor import LLMExtractor
from src.ai_module.result_processor import ResultProcessor
from src.ai_module.ai_database_extension import AIDatabaseExtension
from src.ai_module.config import get_config

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AIJobProcessor:
    """
    AIä½œä¸šå¤„ç†å™¨ä¸»æ§åˆ¶å™¨
    åè°ƒä¸‰é˜¶æ®µæ¶æ„å®Œæˆå®Œæ•´çš„JDä¿¡æ¯æå–æµç¨‹
    """
    
    def __init__(self, 
                 llm_provider: Optional[str] = None,
                 llm_config: Optional[Dict] = None,
                 batch_size: Optional[int] = None):
        """
        åˆå§‹åŒ–AIä½œä¸šå¤„ç†å™¨
        
        Args:
            llm_provider: LLMæä¾›å•† ('mock', 'volcano', 'qwen')ï¼Œå¦‚æœä¸ºNoneåˆ™ä»é…ç½®è¯»å–
            llm_config: LLMé…ç½®å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™ä»é…ç½®è¯»å–
            batch_size: æ‰¹é‡å¤„ç†å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä»é…ç½®è¯»å–
        """
        # åŠ è½½é…ç½®
        self.config = get_config()
        
        # ä½¿ç”¨é…ç½®æˆ–ä¼ å…¥å‚æ•°
        self.llm_provider = llm_provider or self.config.get("llm_extractor", "provider", "mock")
        self.llm_config = llm_config
        self.batch_size = batch_size or self.config.get("data_pipeline", "batch_size", 20)
        
        # åˆå§‹åŒ–ä¸‰é˜¶æ®µç»„ä»¶
        logger.info("åˆå§‹åŒ–AIä½œä¸šå¤„ç†å™¨...")
        
        try:
            # å…±äº«æ•°æ®åº“å®ä¾‹
            self.ai_db = AIDatabaseExtension()
            
            # é˜¶æ®µ1ï¼šæ•°æ®å‡†å¤‡
            self.data_pipeline = DataPipeline(self.ai_db.db_v2)
            
            # é˜¶æ®µ2ï¼šæ™ºèƒ½æå–
            self.llm_extractor = LLMExtractor(self.llm_provider, self.llm_config)
            
            # é˜¶æ®µ3ï¼šç»“æœè¾“å‡º
            self.result_processor = ResultProcessor(self.ai_db)
            
            # å¤„ç†ç»Ÿè®¡
            self.total_sessions = 0
            self.total_processed_messages = 0
            self.total_extracted_jobs = 0
            
            # è®¾ç½®æ—¥å¿—çº§åˆ«
            log_level = self.config.get("system", "log_level", "INFO")
            logging.getLogger().setLevel(getattr(logging, log_level))
            
            logger.info(f"âœ… AIä½œä¸šå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ: provider={self.llm_provider}, batch_size={self.batch_size}")
            
        except Exception as e:
            logger.error(f"âŒ AIä½œä¸šå¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def setup_system(self) -> bool:
        """
        è®¾ç½®ç³»ç»Ÿç¯å¢ƒï¼Œåˆ›å»ºå¿…è¦çš„æ•°æ®åº“è¡¨
        
        Returns:
            è®¾ç½®æˆåŠŸè¿”å›True
        """
        try:
            logger.info("è®¾ç½®AIå¤„ç†ç³»ç»Ÿ...")
            
            # åˆ›å»ºAIä¸“ç”¨æ•°æ®åº“è¡¨
            success = self.ai_db.setup_ai_tables()
            
            if success:
                logger.info("âœ… AIå¤„ç†ç³»ç»Ÿè®¾ç½®å®Œæˆ")
                return True
            else:
                logger.error("âŒ AIå¤„ç†ç³»ç»Ÿè®¾ç½®å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ è®¾ç½®AIå¤„ç†ç³»ç»Ÿæ—¶å‡ºé”™: {e}")
            return False
    
    def process_single_batch(self, batch_size: Optional[int] = None) -> Dict:
        """
        å¤„ç†å•ä¸ªæ‰¹æ¬¡
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        batch_size = batch_size or self.batch_size
        batch_id = f"ai_batch_{int(time.time())}_{self.total_sessions}"
        start_time = time.time()
        
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ‰¹æ¬¡: {batch_id} (å¤§å°={batch_size})")
        
        try:
            # === é˜¶æ®µ1: æ•°æ®å‡†å¤‡ ===
            logger.info("ğŸ“‹ é˜¶æ®µ1: æ•°æ®å‡†å¤‡...")
            stage1_start = time.time()
            
            batch_data = self.data_pipeline.prepare_batch_data(batch_size)
            jd_candidates = batch_data['jd_candidates']
            
            stage1_time = (time.time() - stage1_start) * 1000
            logger.info(f"âœ… é˜¶æ®µ1å®Œæˆ: {len(jd_candidates)} æ¡å€™é€‰æ¶ˆæ¯, è€—æ—¶={stage1_time:.0f}ms")
            
            if not jd_candidates:
                logger.info("æ— å¾…å¤„ç†çš„JDå€™é€‰æ¶ˆæ¯")
                return self._create_empty_result(batch_id, "æ— å¾…å¤„ç†æ¶ˆæ¯")
            
            # === é˜¶æ®µ2: æ™ºèƒ½æå– ===
            logger.info("ğŸ§  é˜¶æ®µ2: æ™ºèƒ½æå–...")
            stage2_start = time.time()
            
            # ä½¿ç”¨æ™ºèƒ½æ‰¹å¤„ç†æ–¹æ³•
            extraction_results = self.llm_extractor.process_batch(jd_candidates)
            
            stage2_time = (time.time() - stage2_start) * 1000
            successful_extractions = len([r for _, r in extraction_results if r is not None])
            
            # è·å–è¯¦ç»†ç»Ÿè®¡
            stats = self.llm_extractor.get_extraction_stats()
            cache_info = f", ç¼“å­˜å‘½ä¸­={stats['cache_hits']}/{stats['total_processed']}" if stats['use_cache'] else ""
            parallel_info = f", å¹¶è¡Œå¤„ç†={stats['parallel_extraction']}" if stats['parallel_extraction'] else ""
            
            log_message = f"âœ… é˜¶æ®µ2å®Œæˆ: {successful_extractions}/{len(extraction_results)} æ¡æˆåŠŸæå–, "
            log_message += f"è€—æ—¶={stage2_time:.0f}ms{cache_info}{parallel_info}"
            logger.info(log_message)
            
            # === é˜¶æ®µ3: ç»“æœè¾“å‡º ===
            logger.info("ğŸ’¾ é˜¶æ®µ3: ç»“æœè¾“å‡º...")
            stage3_start = time.time()
            
            processing_summary = self.result_processor.process_batch_results(extraction_results)
            
            stage3_time = (time.time() - stage3_start) * 1000
            logger.info(f"âœ… é˜¶æ®µ3å®Œæˆ: {processing_summary['saved_count']} æ¡ç»“æœä¿å­˜, è€—æ—¶={stage3_time:.0f}ms")
            
            # === ç”Ÿæˆç»¼åˆç»Ÿè®¡ ===
            total_time = (time.time() - start_time) * 1000
            
            # æ›´æ–°å…¨å±€ç»Ÿè®¡
            self.total_sessions += 1
            self.total_processed_messages += len(batch_data['raw_messages'])
            self.total_extracted_jobs += processing_summary['saved_count']
            
            result = {
                'batch_id': batch_id,
                'success': True,
                'message': 'æ‰¹æ¬¡å¤„ç†å®Œæˆ',
                'timing': {
                    'stage1_data_preparation_ms': int(stage1_time),
                    'stage2_llm_extraction_ms': int(stage2_time),
                    'stage3_result_processing_ms': int(stage3_time),
                    'total_processing_ms': int(total_time)
                },
                'data_flow': {
                    'input_messages': len(batch_data['raw_messages']),
                    'jd_candidates': len(jd_candidates),
                    'extraction_attempts': len(extraction_results),
                    'successful_extractions': successful_extractions,
                    'validated_results': processing_summary['validated_count'],
                    'saved_results': processing_summary['saved_count']
                },
                'quality_metrics': {
                    'filter_ratio': batch_data['filter_ratio'],
                    'extraction_success_rate': successful_extractions / max(len(extraction_results), 1),
                    'validation_success_rate': processing_summary['success_rate'],
                    'average_confidence': processing_summary['average_confidence']
                }
            }
            
            logger.info(f"ğŸ‰ æ‰¹æ¬¡ {batch_id} å¤„ç†å®Œæˆ: "
                       f"è¾“å…¥={result['data_flow']['input_messages']}, "
                       f"è¾“å‡º={result['data_flow']['saved_results']}, "
                       f"è€—æ—¶={result['timing']['total_processing_ms']}ms")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ {batch_id} å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                'batch_id': batch_id,
                'success': False,
                'message': f'æ‰¹æ¬¡å¤„ç†å¤±è´¥: {str(e)}',
                'error': str(e)
            }
    
    def _create_empty_result(self, batch_id: str, message: str) -> Dict:
        """åˆ›å»ºç©ºç»“æœ"""
        return {
            'batch_id': batch_id,
            'success': True,
            'message': message,
            'timing': {'total_processing_ms': 0},
            'data_flow': {
                'input_messages': 0,
                'jd_candidates': 0,
                'saved_results': 0
            },
            'quality_metrics': {
                'filter_ratio': 0.0,
                'extraction_success_rate': 0.0,
                'validation_success_rate': 0.0
            }
        }

# æµ‹è¯•å·¥å…·
def test_ai_job_processor():
    """æµ‹è¯•AI-JDå¤„ç†å™¨"""
    print("=== æµ‹è¯•AI-JDå¤„ç†å™¨ ===")
    
    try:
        # æµ‹è¯•åˆå§‹åŒ–
        print("\n1. æµ‹è¯•åˆå§‹åŒ–:")
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
        processor = AIJobProcessor()
        print("   âœ… AI-JDå¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•ç³»ç»Ÿè®¾ç½®
        print("\n2. æµ‹è¯•ç³»ç»Ÿè®¾ç½®:")
        setup_success = processor.setup_system()
        if setup_success:
            print("   âœ… ç³»ç»Ÿè®¾ç½®æˆåŠŸ")
        else:
            print("   âŒ ç³»ç»Ÿè®¾ç½®å¤±è´¥")
            return False
        
        # æµ‹è¯•å•æ‰¹æ¬¡å¤„ç†
        print("\n3. æµ‹è¯•å•æ‰¹æ¬¡å¤„ç†:")
        batch_result = processor.process_single_batch(batch_size=20)  # å¢åŠ æ‰¹æ¬¡å¤§å°ä»¥åŒ…å«JDæ¶ˆæ¯
        
        if batch_result['success']:
            print("   âœ… å•æ‰¹æ¬¡å¤„ç†æˆåŠŸ:")
            print(f"   - æ‰¹æ¬¡ID: {batch_result['batch_id']}")
            print(f"   - è¾“å…¥æ¶ˆæ¯: {batch_result['data_flow']['input_messages']}")
            print(f"   - JDå€™é€‰: {batch_result['data_flow']['jd_candidates']}")
            print(f"   - ä¿å­˜ç»“æœ: {batch_result['data_flow']['saved_results']}")
            print(f"   - æ€»è€—æ—¶: {batch_result['timing']['total_processing_ms']}ms")
            
            # æ˜¾ç¤ºé˜¶æ®µè€—æ—¶
            timing = batch_result['timing']
            if 'stage1_data_preparation_ms' in timing:
                print(f"   - é˜¶æ®µ1è€—æ—¶: {timing['stage1_data_preparation_ms']}ms")
                print(f"   - é˜¶æ®µ2è€—æ—¶: {timing['stage2_llm_extraction_ms']}ms")
                print(f"   - é˜¶æ®µ3è€—æ—¶: {timing['stage3_result_processing_ms']}ms")
            
            # æ˜¾ç¤ºè´¨é‡æŒ‡æ ‡
            quality = batch_result['quality_metrics']
            print(f"   - è¿‡æ»¤æ¯”ä¾‹: {quality.get('filter_ratio', 0.0):.2%}")
            print(f"   - æå–æˆåŠŸç‡: {quality.get('extraction_success_rate', 0.0):.2%}")
            print(f"   - éªŒè¯æˆåŠŸç‡: {quality.get('validation_success_rate', 0.0):.2%}")
            print(f"   - å¹³å‡ç½®ä¿¡åº¦: {quality.get('average_confidence', 0.0):.2f}")
        else:
            print(f"   âŒ å•æ‰¹æ¬¡å¤„ç†å¤±è´¥: {batch_result['message']}")
        
        print("\nâœ… AIä½œä¸šå¤„ç†å™¨æµ‹è¯•å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"\nâŒ AIä½œä¸šå¤„ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_ai_job_processor()
 